{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal no., email** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   ** Armand Ghaffarpour, 9101103738, armandg@student.chalmers.se**  <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   ** Ryan Damarputra Widjaja, 9002205616, ryand@student.chalmers.se** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n",
    "\n",
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#gaussian multivariate\n",
    "def gaussian(X,mu,sigma):\n",
    "    p = X.shape[1]\n",
    "    factor = np.divide(1,np.power((2*np.pi),np.divide(p,2))*np.power(sigma,p))\n",
    "    expo = np.exp(np.divide(np.linalg.norm(X-mu),-2*np.power(sigma,2)))\n",
    "    prob = factor * expo\n",
    "    return prob\n",
    "\n",
    "#bayes classifier\n",
    "def sph_bayes(Xtest, mu1, sigma1, mu2, sigma2):\n",
    "    p1 = gaussian(Xtest,mu1,sigma1)\n",
    "    p2 = gaussian(Xtest,mu2,sigma2)\n",
    "    if(p1 > p2):\n",
    "        ytest = \"+1\"\n",
    "    else:\n",
    "        ytest = \"-1\"\n",
    "    return [p1, p2, ytest]\n",
    "\n",
    "#new_classifier\n",
    "def new_classifier(Xtest,mu1,mu2):\n",
    "    b = (mu1 + mu2) / 2\n",
    "    mudif = (mu1-mu2).T\n",
    "    multiplier = Xtest - b\n",
    "    length = np.linalg.norm(mu1-mu2)\n",
    "    prob = np.divide(np.dot(mudif,multiplier),length)\n",
    "    ytest = np.sign(prob)\n",
    "    return ytest\n",
    "\n",
    "#MLE for mu and sigma\n",
    "def sge(X):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    xlist = X[:,0]\n",
    "    ylist = X[:,1]\n",
    "    zlist = X[:,2]\n",
    "    \n",
    "    mu = np.sum(X,axis=0)/n\n",
    "    stemp = 0\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        #stemp += np.dot(np.transpose(X[i]-mu),X[i]-mu)\n",
    "        x = np.array([xlist[i],ylist[i],zlist[i]])\n",
    "        difference = np.power(np.linalg.norm(x-mu),2)\n",
    "        res = np.divide(difference,2)\n",
    "        stemp += res\n",
    "\n",
    "    sigma = np.sqrt(stemp / (n*p))\n",
    "    return mu,sigma\n",
    "\n",
    "#split the data between class +1 and class -1\n",
    "def splitter(X):\n",
    "    n = X.shape[0]\n",
    "    s1 = np.zeros(shape=(1000,3))\n",
    "    s2 = np.zeros(shape=(1000,3))\n",
    "    for i in range (0,n):\n",
    "        index = i\n",
    "        x1 = X[i][0]\n",
    "        x2 = X[i][1]\n",
    "        x3 = X[i][2]\n",
    "        cls = X[i][3]\n",
    "        if (i>=1000):\n",
    "            index -= 1000\n",
    "        if(cls==1):\n",
    "            s1[index][0] = x1\n",
    "            s1[index][1] = x2\n",
    "            s1[index][2] = x3\n",
    "        else:\n",
    "            s2[index][0] = x1\n",
    "            s2[index][1] = x2\n",
    "            s2[index][2] = x3\n",
    "    return s1,s2\n",
    "\n",
    "#main\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "test = np.hsplit(data,np.array([3,6]))\n",
    "\n",
    "trainingdata = test[0]\n",
    "targetdata = test[1]\n",
    "\n",
    "labels = data[:,-1]\n",
    "class1, classMinus1 = splitter(data)\n",
    "\n",
    "mu1, sigma1 = sge(class1)\n",
    "mu2, sigma2 = sge(classMinus1)\n",
    "\n",
    "nt = trainingdata.shape[0]\n",
    "binclf = np.zeros(shape=(nt,1))\n",
    "#gausclf = np.zeros(shpae=(nt,3))\n",
    "\n",
    "# TODO : how to perform cross validation\n",
    "\n",
    "for i in range(0,nt):\n",
    "    xtest = trainingdata[i]\n",
    "    classtest = new_classifier(xtest,mu1,mu2)\n",
    "    #gaustest = sph_bayes(xtest,mu1,sigma1,mu2,sigma2)\n",
    "    #binclf[i] = classtest\n",
    "    #print(gaustest)\n",
    "\n",
    "#print(gausclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  2. 10.  7.  0.  0.  0.]\n",
      " [ 0.  0. 14. 16. 16. 15.  1.  0.]\n",
      " [ 0.  4. 16.  7.  3. 16.  7.  0.]\n",
      " [ 0.  5. 16. 10.  7. 16.  4.  0.]\n",
      " [ 0.  0.  5. 14. 14. 16.  4.  0.]\n",
      " [ 0.  0.  0.  0.  0. 16.  2.  0.]\n",
      " [ 0.  0.  4.  7.  7. 16.  2.  0.]\n",
      " [ 0.  0.  5. 12. 16. 12.  0.  0.]]\n",
      "-1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAC/9JREFUeJzt3X+s1XUdx/HXywuIIEILcw78kbOxnKUwhjqWK1CH5Ui3/oBNt1iNZuVktTmtP9K2/nX2R3Nz/lwizl+0ZmWy1JnLMEAIEHRKmDfUq3OGgIHguz/Ol0bs5v1eup/PPee+n4/tjHPvPfe83hd4ne/3nPs9348jQgByOW60BwBQH8UHEqL4QEIUH0iI4gMJUXwgoa4ovu1Ftl+2/artGwtn3W17wPaWkjlH5J1m+2nb22xvtX194byJtl+wvanJu6VkXpPZZ/tF24+XzmrydtrebHuj7XWFs6bZfsT29ubf8KKCWbOan+nwZbftFUXCImJUL5L6JL0m6SxJEyRtknROwbyLJc2RtKXSz3eqpDnN9SmSXin881nSic318ZLWSrqw8M/4A0kPSHq80t/pTknTK2XdJ+nbzfUJkqZVyu2T9JakM0rcfzds8edJejUidkTEAUkPSvp6qbCIeFbSe6Xuf5C8NyNiQ3P9A0nbJM0omBcRsaf5cHxzKXaUlu2Zkr4m6c5SGaPF9knqbCjukqSIOBAR71eKXyjptYh4vcSdd0PxZ0h644iP+1WwGKPJ9pmSZquzFS6Z02d7o6QBSWsiomTebZJukPRxwYyjhaQnba+3vbxgzlmS3pF0T/NU5k7bkwvmHWmJpFWl7rwbiu9BPjfmjiO2faKkRyWtiIjdJbMi4lBEnC9ppqR5ts8tkWP7CkkDEbG+xP1/gvkRMUfS5ZK+Z/viQjnj1HlaeHtEzJa0V1LR16AkyfYESYslPVwqoxuK3y/ptCM+nilp1yjNUoTt8eqUfmVEPFYrt9ktfUbSokIR8yUttr1TnadoC2zfXyjrPyJiV/PngKTV6jxdLKFfUv8Re0yPqPNAUNrlkjZExNulArqh+H+R9Dnbn20e6ZZI+vUozzRibFud54jbIuLWCnkn257WXD9B0iWStpfIioibImJmRJypzr/bUxFxdYmsw2xPtj3l8HVJl0kq8huaiHhL0hu2ZzWfWijppRJZR1mqgrv5UmdXZlRFxEHb35f0e3Veybw7IraWyrO9StKXJU233S/pJxFxV6k8dbaK10ja3DzvlqQfRcRvC+WdKuk+233qPLA/FBFVfs1WySmSVnceTzVO0gMR8UTBvOskrWw2SjskLSuYJduTJF0q6TtFc5pfHQBIpBt29QFURvGBhCg+kBDFBxKi+EBCXVX8wodfjloWeeR1W15XFV9Szb/cqv+Q5JHXTXndVnwAFRQ5gGeCj4+JGv6bmD7Sfo3X8SM+z0hneeLwv+/AoX2a0DfpmPIOzBjsfUyf7ODufRp30rHlxYd9w/6eQ3v3qm/ysb1xbcKuvcP+npr/V3op71/aqwOxf8j/MEUO2Z2oybrAC0vcdVfoO3vW0DcaQa//tO6R1R9vnlo17/Sb/1Q1byxbG39odTt29YGEKD6QEMUHEqL4QEIUH0iI4gMJUXwgIYoPJNSq+DWXuAJQ3pDFb07a+At1Tvl7jqSlts8pPRiActps8asucQWgvDbFT7PEFZBFm3d/tFriqjlxwHJJmqhje1cYgDrabPFbLXEVEXdExNyImFvz7YsAhq9N8cf0EldARkPu6tde4gpAea3O8NCs81ZqrTcAlXHkHpAQxQcSovhAQhQfSIjiAwlRfCAhig8kRPGBhOou0TJGLP/Vb6rmXTl5T9U8XVQ37pVlw19C6/+xYt5VVfMOvT1QNa8NtvhAQhQfSIjiAwlRfCAhig8kRPGBhCg+kBDFBxKi+EBCFB9IqM0SWnfbHrC9pcZAAMprs8W/V9KiwnMAqGjI4kfEs5LeqzALgEp4jg8kNGJvy2XtPKB3jNgWn7XzgN7Brj6QUJtf562S9LykWbb7bX+r/FgASmqzaObSGoMAqIddfSAhig8kRPGBhCg+kBDFBxKi+EBCFB9IiOIDCY2JtfP2XXVB1bwrJ2+smvf5O75bNW/m0x9WzVuz6p6qeX+79uyqeaffzNp5ALoAxQcSovhAQhQfSIjiAwlRfCAhig8kRPGBhCg+kBDFBxJqc7LN02w/bXub7a22r68xGIBy2hyrf1DSDyNig+0pktbbXhMRLxWeDUAhbdbOezMiNjTXP5C0TdKM0oMBKGdYz/FtnylptqS1JYYBUEfrt+XaPlHSo5JWRMTuQb7O2nlAj2i1xbc9Xp3Sr4yIxwa7DWvnAb2jzav6lnSXpG0RcWv5kQCU1maLP1/SNZIW2N7YXL5aeC4ABbVZO+85Sa4wC4BKOHIPSIjiAwlRfCAhig8kRPGBhCg+kBDFBxKi+EBCY2LtvP1Tx/bj13Ff+GfVvH5NrZpX2/RNh0Z7hFE3thsDYFAUH0iI4gMJUXwgIYoPJETxgYQoPpAQxQcSovhAQhQfSKjNWXYn2n7B9qZm7bxbagwGoJw2x+rvl7QgIvY059d/zvbvIuLPhWcDUEibs+yGpD3Nh+ObS5QcCkBZbVfS6bO9UdKApDURwdp5QA9rVfyIOBQR50uaKWme7XOPvo3t5bbX2V73kfaP9JwARtCwXtWPiPclPSNp0SBfY+08oEe0eVX/ZNvTmusnSLpE0vbSgwEop82r+qdKus92nzoPFA9FxONlxwJQUptX9f8qaXaFWQBUwpF7QEIUH0iI4gMJUXwgIYoPJETxgYQoPpAQxQcSGhNr533q3uer5s3TtVXzfvbjX1bN0xfrxqE+tvhAQhQfSIjiAwlRfCAhig8kRPGBhCg+kBDFBxKi+EBCFB9IqHXxm0U1XrTNiTaBHjecLf71kraVGgRAPW2X0Jop6WuS7iw7DoAa2m7xb5N0g6SPC84CoJI2K+lcIWkgItYPcTvWzgN6RJst/nxJi23vlPSgpAW27z/6RqydB/SOIYsfETdFxMyIOFPSEklPRcTVxScDUAy/xwcSGtaptyLiGXWWyQbQw9jiAwlRfCAhig8kRPGBhCg+kBDFBxKi+EBCFB9IaEysnVdb7bX6br/37Kp5tV25a2PVvCmvvF8171DVtHbY4gMJUXwgIYoPJETxgYQoPpAQxQcSovhAQhQfSIjiAwlRfCChVofsNqfW/kCdow8PRsTckkMBKGs4x+p/JSLeLTYJgGrY1QcSalv8kPSk7fW2l5ccCEB5bXf150fELtufkbTG9vaIePbIGzQPCMslaaImjfCYAEZSqy1+ROxq/hyQtFrSvEFuw9p5QI9os1ruZNtTDl+XdJmkLaUHA1BOm139UySttn349g9ExBNFpwJQ1JDFj4gdks6rMAuASvh1HpAQxQcSovhAQhQfSIjiAwlRfCAhig8kRPGBhFg77xjsu+qCqnnvntdXNa++umvngS0+kBLFBxKi+EBCFB9IiOIDCVF8ICGKDyRE8YGEKD6QEMUHEmpVfNvTbD9ie7vtbbYvKj0YgHLaHqv/c0lPRMQ3bE+QWDED6GVDFt/2SZIulvRNSYqIA5IOlB0LQEltdvXPkvSOpHtsv2j7zmZhjf9ie7ntdbbXfaT9Iz4ogJHTpvjjJM2RdHtEzJa0V9KNR9+IJbSA3tGm+P2S+iNibfPxI+o8EADoUUMWPyLekvSG7VnNpxZKeqnoVACKavuq/nWSVjav6O+QtKzcSABKa1X8iNgoaW7hWQBUwpF7QEIUH0iI4gMJUXwgIYoPJETxgYQoPpAQxQcSYu28Y7B/at3HywsXba6ad8/pf6yat+zvX6qad2jry1XzuhFbfCAhig8kRPGBhCg+kBDFBxKi+EBCFB9IiOIDCVF8IKEhi297lu2NR1x2215RYzgAZQx5yG5EvCzpfEmy3SfpH5JWF54LQEHD3dVfKOm1iHi9xDAA6hhu8ZdIWlViEAD1tC5+c079xZIe/h9fZ+08oEcMZ4t/uaQNEfH2YF9k7Tygdwyn+EvFbj4wJrQqvu1Jki6V9FjZcQDU0HYJrX2SPl14FgCVcOQekBDFBxKi+EBCFB9IiOIDCVF8ICGKDyRE8YGEKD6QkCNi5O/UfkfSsbxnf7qkd0d4nG7IIo+8WnlnRMTJQ92oSPGPle11ETF3rGWRR1635bGrDyRE8YGEuq34d4zRLPLI66q8rnqOD6CObtviA6iA4gMJUXwgIYoPJETxgYT+DaTarKXUNbVxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assume +1 = 5, and -1 = 8\n",
    "# new_classifier from previous question\n",
    "def new_classifier(Xtest,mu1,mu2):\n",
    "    b = (mu1 + mu2) / 2\n",
    "    mudif = (mu1-mu2).T\n",
    "    multiplier = Xtest - b\n",
    "    length = np.linalg.norm(mu1-mu2)\n",
    "    prob = np.divide(np.dot(mudif,multiplier),length)\n",
    "    ytest = np.sign(prob)\n",
    "    return ytest\n",
    "\n",
    "# main\n",
    "# variable \"data\" represents an image\n",
    "# the image's shape can be seen in y\n",
    "# example : if y[1] has 3 as its value, then data[1] will be shaped like a \"3\"\n",
    "# values in the \"data\" variable corresponds to the color of a pixel, ranging from 0-255\n",
    "# will be easier to look if we perform data.reshape(8,8)\n",
    "\n",
    "# TODO : scale data from 0-255 to 0-1\n",
    "# TODO : calculate variance for all the rows and columns. So there will bi 8 variances for each row, and 8 variances for each column\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "print(data[1795].reshape(8,8))\n",
    "\n",
    "target_names = digits.target_names\n",
    "#print (target_names)\n",
    "\n",
    "n = y.shape[0]\n",
    "class5 = []\n",
    "class8 = []\n",
    "\n",
    "for i in range(0,n):\n",
    "    if (y[i] == 5):\n",
    "        class5.append(data[i])\n",
    "    elif(y[i] == 8):\n",
    "        class8.append(data[i])\n",
    "        \n",
    "mu1 = np.mean(class5,axis=0)\n",
    "mu2 = np.mean(class8,axis=0)\n",
    "\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[1795])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
