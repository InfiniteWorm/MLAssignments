{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal no., email** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   ** Armand Ghaffarpour, 9101103738, armandg@student.chalmers.se**  <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   ** Ryan Damarputra Widjaja, 9002205616, ryand@student.chalmers.se** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n",
    "\n",
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Error of new_classifier is :  0.0\n",
      "The Average Error of sph_bayes is :  0.0\n"
     ]
    }
   ],
   "source": [
    "######################################### Practical Problems #########################################\n",
    "########################################## Bayes Classifier ##########################################\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#gaussian multivariate\n",
    "def gaussian(X,mu,sigma):\n",
    "    p = X.shape[0]\n",
    "    factor = np.divide(1,np.power((2*np.pi),np.divide(p,2))*np.power(sigma,p))\n",
    "    expo = np.exp(np.divide(np.dot(np.transpose(X-mu),X-mu),-2*np.power(sigma,2)))\n",
    "    prob = factor * expo\n",
    "    return prob\n",
    "\n",
    "#bayes classifier\n",
    "def sph_bayes(Xtest, mu1, sigma1, mu2, sigma2):\n",
    "    p1 = gaussian(Xtest,mu1,sigma1) / gaussian(Xtest,mu1,sigma1) + gaussian(Xtest,mu2,sigma2)\n",
    "    p2 = gaussian(Xtest,mu2,sigma2) / gaussian(Xtest,mu1,sigma1) + gaussian(Xtest,mu2,sigma2)\n",
    "    if(p1 > p2):\n",
    "        ytest = 1\n",
    "    else:\n",
    "        ytest = -1\n",
    "    return [p1, p2, ytest]\n",
    "\n",
    "#new_classifier\n",
    "def new_classifier(Xtest,mu1,mu2):\n",
    "    b = (mu1 + mu2) / 2\n",
    "    mudif = (mu1-mu2).T\n",
    "    multiplier = Xtest - b\n",
    "    length = np.linalg.norm(mu1-mu2)\n",
    "    prob = np.divide(np.dot(mudif,multiplier),length)\n",
    "    ytest = np.sign(prob)\n",
    "    return ytest\n",
    "\n",
    "#MLE for mu and sigma\n",
    "def sge(X):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    xlist = X[:,0]\n",
    "    ylist = X[:,1]\n",
    "    zlist = X[:,2]\n",
    "    \n",
    "    mu = np.sum(X,axis=0)/n\n",
    "    stemp = 0\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        #stemp += np.dot(np.transpose(X[i]-mu),X[i]-mu)\n",
    "        x = np.array([xlist[i],ylist[i],zlist[i]])\n",
    "        difference = np.power(np.linalg.norm(x-mu),2)\n",
    "        res = np.divide(difference,2)\n",
    "        stemp += res\n",
    "\n",
    "    sigma = np.sqrt(stemp / (n*p))\n",
    "    return mu,sigma\n",
    "\n",
    "#split the data between class +1 and class -1\n",
    "def splitter2(X):\n",
    "    n = X.shape[0]\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for i in range (0,n):\n",
    "        x1 = X[i][0]\n",
    "        x2 = X[i][1]\n",
    "        x3 = X[i][2]\n",
    "        cls = X[i][3]\n",
    "        if(cls==1):\n",
    "            s1.append([x1,x2,x3])\n",
    "        else:\n",
    "            s2.append([x1,x2,x3])\n",
    "    ss1 = np.array(s1).reshape(len(s1),3)\n",
    "    ss2 = np.array(s2).reshape(len(s2),3)\n",
    "    return ss1,ss2\n",
    "\n",
    "\n",
    "def splitter(X):\n",
    "    n = X.shape[0]\n",
    "    s1 = np.zeros(shape=(1000,3))\n",
    "    s2 = np.zeros(shape=(1000,3))\n",
    "    for i in range (0,n):\n",
    "        index = i\n",
    "        x1 = X[i][0]\n",
    "        x2 = X[i][1]\n",
    "        x3 = X[i][2]\n",
    "        cls = X[i][3]\n",
    "        if (i>=1000):\n",
    "            index -= 1000\n",
    "        if(cls==1):\n",
    "            s1[index][0] = x1\n",
    "            s1[index][1] = x2\n",
    "            s1[index][2] = x3\n",
    "        else:\n",
    "            s2[index][0] = x1\n",
    "            s2[index][1] = x2\n",
    "            s2[index][2] = x3\n",
    "    return s1,s2\n",
    "\n",
    "#main\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "\n",
    "#split data into 2 parts : datapoint, and datalabels\n",
    "splitdata = np.split(data,np.array([3,4]),axis=1)\n",
    "datapoint = splitdata[0]\n",
    "datalabels = splitdata[1]\n",
    "\n",
    "# declare that kf will be a StratifiedKFold, to ensure that both of the classes will be distributed equally\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "\n",
    "# container for calculating the errors\n",
    "errC = []\n",
    "errG = []\n",
    "\n",
    "# perform the k-fold cross validation with the given data\n",
    "for train_index, test_index in kf.split(datapoint,datalabels):\n",
    "    \n",
    "    # training data\n",
    "    train = datapoint[train_index]\n",
    "    \n",
    "    # testing data\n",
    "    test = datapoint[test_index]\n",
    "    \n",
    "    # expected result\n",
    "    expectedResult = datalabels[test_index]\n",
    "    \n",
    "    # split training data by their class\n",
    "    newdata = data[train_index]\n",
    "    class1, classMinus1 = splitter2(newdata)\n",
    "    \n",
    "    # estimating mu and sigma for both classes\n",
    "    mu1, sigma1 = sge(class1)\n",
    "    mu2, sigma2 = sge(classMinus1)\n",
    "    \n",
    "    # initializing error count\n",
    "    classerror = 0\n",
    "    gausserror = 0\n",
    "    \n",
    "    # for each test data, classify them both with new_classifier and sph_bayes\n",
    "    # and compare the result to the expected result\n",
    "    nt = test.shape[0]\n",
    "    for i in range(0,nt):\n",
    "        xtest = test[i]\n",
    "        classtest = new_classifier(xtest,mu1,mu2)\n",
    "        gausstest = sph_bayes(xtest,mu1,sigma1,mu2,sigma2)  \n",
    "        if(expectedResult[i] != classtest):\n",
    "            classerror += 1\n",
    "        if(expectedResult[i] != gausstest[2]):\n",
    "            gausserror += 1\n",
    "         \n",
    "    errC.append(classerror)\n",
    "    errG.append(gausserror)\n",
    "\n",
    "averageErrorC = np.average(errC)\n",
    "averageErrorG = np.average(errG)\n",
    "\n",
    "print(\"The Average Error of new_classifier is : \", averageErrorC)\n",
    "print(\"The Average Error of sph_bayes is : \", averageErrorG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Problem - Bayes Classifier Observation\n",
    "\n",
    "We are using StratifiedKFold function, which function like a normal KFold function, but with a slight differece : StratifiedKFold ensures that every class is evenly distributed both in the test data and the training data.\n",
    "\n",
    "After performing 5-fold cross validation, we observe that both of them does not have any error in classifying the data. That means they are equally the same, and in this problem we can use either one of them to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.39, 1.39, 1.41, 2.82, 1.43]\n",
      "[12.5, 19.44, 11.27, 11.27, 11.43]\n",
      "Average error with new_classifier : 1.69 %\n",
      "Average error with variance data : 13.18 %\n"
     ]
    }
   ],
   "source": [
    "######################################### Practical Problems #########################################\n",
    "###################################### Digits dataset classifier #####################################\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# assume +1 = 8, and -1 = 5\n",
    "# new_classifier from previous question\n",
    "def new_classifier(Xtest,mu1,mu2):\n",
    "    b = (mu1 + mu2) / 2\n",
    "    mudif = (mu1-mu2).T\n",
    "    multiplier = Xtest - b\n",
    "    length = np.linalg.norm(mu1-mu2)\n",
    "    prob = np.sign(np.divide(np.dot(mudif,multiplier),length))\n",
    "    if(prob==1):\n",
    "        ytest = 5\n",
    "    else:\n",
    "        ytest = 8\n",
    "    return ytest\n",
    "\n",
    "def splitter(X,y):\n",
    "    n = X.shape[0]\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for i in range (0,n):\n",
    "        if(y[i]==5):\n",
    "            s1.append(X[i])\n",
    "        elif(y[i]==8):\n",
    "            s2.append(X[i])\n",
    "    ss1 = np.array(s1).reshape(len(s1),X.shape[1])\n",
    "    ss2 = np.array(s2).reshape(len(s2),X.shape[1])\n",
    "    return ss1,ss2\n",
    "\n",
    "#function to normalize data between 0 and 1 using min-max normalization method\n",
    "def normalizer(dat):\n",
    "    maxval = np.amax(dat,axis=1)\n",
    "    minval = np.amin(dat,axis=1)\n",
    "    result = np.zeros(shape=(dat.shape[0],dat.shape[1]))\n",
    "    #result = (dat-minval.reshape(12,1))/(maxval-minval).reshape(12,1)\n",
    "    rows = dat.shape[0]\n",
    "    cols = dat.shape[1]\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0,cols):\n",
    "            if(maxval[i]==0 and minval[i] ==0):\n",
    "                result[i][j]=0\n",
    "            else:\n",
    "                result[i][j] = (dat[i][j] - minval[i]) / (maxval[i]-minval[i])\n",
    "    return result\n",
    "\n",
    "# k-fold cross validation function\n",
    "def crossValidation(splits,boolShuffle,data,labels):\n",
    "    kf = StratifiedKFold(n_splits=splits,shuffle=boolShuffle)\n",
    "    errRes = []\n",
    "    for train_index, test_index in kf.split(data,labels):\n",
    "        traindata = data[train_index]\n",
    "        trainres = labels[train_index]\n",
    "        testdata = data[test_index]\n",
    "        expectedResult = labels[test_index]\n",
    "        class5, class8 = splitter(traindata,trainres)\n",
    "        \n",
    "        mu1 = np.mean(class5,axis=0)\n",
    "        mu2 = np.mean(class8,axis=0)\n",
    "        \n",
    "        error = 0\n",
    "        nt = testdata.shape[0]\n",
    "        \n",
    "        for i in range(0,nt):\n",
    "            xtest = testdata[i]\n",
    "            classtest = new_classifier(xtest,mu1,mu2)\n",
    "            if(expectedResult[i] != classtest):\n",
    "                error+=1\n",
    "        errRes.append(round(((error/nt)*100),2))\n",
    "    return errRes\n",
    "\n",
    "# main\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "target_names = digits.target_names\n",
    "y = digits.target\n",
    "n = y.shape[0]\n",
    "\n",
    "filteredData = []\n",
    "filteredLabels = []\n",
    "\n",
    "# filtering the data so that it contains only the images with digits 5 and 8 in it.\n",
    "for i in range(0,n):\n",
    "    if (y[i] == 5 or y[i]==8):\n",
    "        filteredData.append(data[i])\n",
    "        filteredLabels.append(y[i])\n",
    "\n",
    "normData = normalizer(np.array(filteredData).reshape(len(filteredLabels),64))\n",
    "varianceData = []\n",
    "\n",
    "for i in range(0,normData.shape[0]):\n",
    "    x = normData[i].reshape(8,8)\n",
    "    \n",
    "    feature = []\n",
    "    colVariance = np.var(x,axis=0) #variance along the columns\n",
    "    rowVariance = np.var(x,axis=1) #variance along the rows\n",
    "    feature.extend(rowVariance)\n",
    "    feature.extend(colVariance)\n",
    "    varianceData.append(feature)\n",
    "    \n",
    "newData = np.array(varData).reshape(normData.shape[0],16)\n",
    "errC = crossValidation(5,True,np.array(filteredData).reshape(len(filteredData),64),np.array(filteredLabels).reshape(len(filteredLabels),1))\n",
    "errV = crossValidation(5,True,newData,np.array(filteredLabels).reshape(len(filteredLabels),1))\n",
    "\n",
    "print(errC)\n",
    "print(errV)\n",
    "print(\"Average error with new_classifier : {0} %\".format(round(np.average(errC),2)))\n",
    "print(\"Average error with variance data : {0} %\".format(round(np.average(errV),2)))\n",
    "\n",
    "#plt.matshow(digits.images[1795])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Problem - DIGITS dataset classifier\n",
    "\n",
    "By doing Cross-validation on both (a) and (b), we have this result : \n",
    "\n",
    "| Class         | Fold #1       | Fold #2  | Fold #3 | Fold #4 | Fold #5| Average|\n",
    "| ------------- |:-------------:| --------:|:-------:|:-------:|:------:|:-------:|\n",
    "| new_classifier| 1.39% | 1.39% | 1.41% | 2.82% | 1.43% | 1.69% |\n",
    "| variance      | 12.5% | 19.44% | 11.27% | 11.27% | 11.43% | 13.18% |\n",
    "\n",
    "Of course, this result will be different when we run the code again, because we are setting the Shuffle flag to True when calling the StratifiedKFold function. But what we observe here is that the error rate for new_classifier with the unmodified data (aside from filtering so that now it contains only 5 and 8 digits) is very small. This tells us that the new_classifier might be suitable for the problem. \n",
    "\n",
    "By modifying our data to using variance as the features, the error rates has increased.\n",
    "\n",
    "However in here we assume that the class +1 is digit 5, and class -1 is digit 8. We found out that just by switching the labels (class +1 is 8 and class -1 is 5), the error rates is significantly increased, around 90% or so. Since this is not specified in the question, we can't decide which one is the correct one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
